{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extração Automatizada de Dados - Agentes Inteligentes INF/UFG\n",
        "### Prof. Otávio Calaça Xavier\n",
        "----\n",
        "\n",
        "## BeautifulSoup"
      ],
      "metadata": {
        "id": "Wf49sq3L-PAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo Prático com Código\n",
        "\n",
        "A seguir, vamos ver um script básico que:\n",
        "\n",
        "1. Solicita uma página via requests\n",
        "\n",
        "1. Faz o parsing com BeautifulSoup\n",
        "\n",
        "1. Localiza elementos específicos (títulos, preços, avaliações)\n",
        "\n",
        "1. Exibe no console quantos itens foram encontrados\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XKWg0Xhh-m4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://books.toscrape.com\"\n",
        "html = requests.get(url, timeout=10).text\n",
        "soup = BeautifulSoup(html, \"lxml\")  # ou \"html.parser\"\n",
        "\n",
        "# Encontrar todos os títulos\n",
        "titles = [h3.a[\"title\"] for h3 in soup.select(\"article.product_pod h3\")]\n",
        "print(len(titles), \"livros encontrados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEu_cNBY-ltf",
        "outputId": "5c9c75c0-ab13-4f78-ed96-438eba39b3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A Light in the Attic', 'Tipping the Velvet', 'Soumission', 'Sharp Objects', 'Sapiens: A Brief History of Humankind', 'The Requiem Red', 'The Dirty Little Secrets of Getting Your Dream Job', 'The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull', 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics', 'The Black Maria', 'Starving Hearts (Triangular Trade Trilogy, #1)', \"Shakespeare's Sonnets\", 'Set Me Free', \"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\", 'Rip it Up and Start Again', 'Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991', 'Olio', 'Mesaerion: The Best Science Fiction Stories 1800-1849', 'Libertarianism for Beginners', \"It's Only the Himalayas\"]\n",
            "20 livros encontrados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2 — Extraindo Todos os Links de uma Página\n",
        "\n",
        "Vamos buscar todos os URLs (atributo href) encontrados em tags `<a>` de uma página e exibi-los em uma lista. Isso pode ser útil para coletar todas as páginas de um determinado domínio ou encontrar links de download."
      ],
      "metadata": {
        "id": "yJBnDa9V_D2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Definir a URL que queremos analisar\n",
        "url = \"https://g1.globo.com\"\n",
        "\n",
        "# Fazer a requisição HTTP para obter o HTML\n",
        "response = requests.get(url, timeout=10)\n",
        "html_content = response.text\n",
        "\n",
        "# Criar o objeto BeautifulSoup usando o parser lxml\n",
        "soup = BeautifulSoup(html_content, \"lxml\")\n",
        "\n",
        "# Encontrar todas as tags <a> que possuem atributo 'href'\n",
        "links = []\n",
        "for tag in soup.find_all(\"a\", href=True):\n",
        "    href = tag[\"href\"]\n",
        "    # (Opcional) Filtrar apenas links que começam com \"http\"\n",
        "    if href.startswith(\"http\"):\n",
        "        links.append(href)\n",
        "\n",
        "# Exibir quantos links foram encontrados e imprimir alguns exemplos\n",
        "print(f\"Total de links absolutos encontrados: {len(links)}\")\n",
        "print(\"Exemplos de links:\")\n",
        "for link in links[:50]:\n",
        "    print(\" •\", link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtI1adKbAPFU",
        "outputId": "80679899-c401-4f73-d48d-afb3c6228ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de links absolutos encontrados: 682\n",
            "Exemplos de links:\n",
            " • https://g1.globo.com/\n",
            " • https://g1.globo.com/politica/noticia/2025/05/31/hugo-motta-cobra-suspensao-imediata-do-decreto-do-iof-sobre-risco-sacado.ghtml\n",
            " • https://g1.globo.com/politica/noticia/2025/05/31/hugo-motta-cobra-suspensao-imediata-do-decreto-do-iof-sobre-risco-sacado.ghtml\n",
            " • https://g1.globo.com/rj/rio-de-janeiro/noticia/2025/05/31/ataques-a-provedores-de-internet-no-ceara-vieram-do-rio.ghtml\n",
            " • https://g1.globo.com/rj/rio-de-janeiro/noticia/2025/05/31/ataques-a-provedores-de-internet-no-ceara-vieram-do-rio.ghtml\n",
            " • https://g1.globo.com/ce/ceara/noticia/2025/05/31/sem-internet-e-empresas-falidas-o-impacto-dos-ataques-a-provedores-no-ceara.ghtml\n",
            " • https://g1.globo.com/rj/rio-de-janeiro/noticia/2025/05/31/traficantes-do-ceara-escondidos-na-rocinha.ghtml\n",
            " • https://ge.globo.com/futebol/futebol-internacional/liga-dos-campeoes/jogo/31-05-2025/parissaintgermain-interdemilao.ghtml\n",
            " • https://ge.globo.com/futebol/futebol-internacional/liga-dos-campeoes/jogo/31-05-2025/parissaintgermain-interdemilao.ghtml\n",
            " • https://g1.globo.com/sp/sao-paulo/noticia/2025/05/31/a-justica-ela-foi-feita-diz-isabela-ticherani-filha-de-paulo-cupertino-apos-a-condenacao-do-pai-por-matar-a-tiros-seu-namorado-e-pais-dele-em-sp.ghtml\n",
            " • https://g1.globo.com/sp/sao-paulo/noticia/2025/05/31/a-justica-ela-foi-feita-diz-isabela-ticherani-filha-de-paulo-cupertino-apos-a-condenacao-do-pai-por-matar-a-tiros-seu-namorado-e-pais-dele-em-sp.ghtml\n",
            " • https://g1.globo.com/sp/sao-paulo/noticia/2025/05/30/paulo-cupertino-e-condenado-por-matar-a-tiros-ator-e-pais-dele-em-sp.ghtml\n",
            " • https://g1.globo.com/politica/noticia/2025/05/31/exame-toxicologico-nao-sera-exigido-para-renovar-cnh-de-carro-e-moto.ghtml\n",
            " • https://g1.globo.com/politica/noticia/2025/05/31/exame-toxicologico-nao-sera-exigido-para-renovar-cnh-de-carro-e-moto.ghtml\n",
            " • https://g1.globo.com/carros/noticia/2025/05/31/maio-amarelo-veja-dicas-de-direcao-defensiva-essenciais-para-sua-seguranca-no-transito.ghtml\n",
            " • https://g1.globo.com/mundo/noticia/2025/05/31/netanyahu-resposta-do-hamas-a-acordo-de-cessar-fogo-e-inaceitavel.ghtml\n",
            " • https://g1.globo.com/mundo/noticia/2025/05/31/netanyahu-resposta-do-hamas-a-acordo-de-cessar-fogo-e-inaceitavel.ghtml\n",
            " • https://g1.globo.com/sp/sao-paulo/noticia/2025/05/31/apos-10-meses-justica-manda-soltar-motorista-do-porsche-amarelo-acusado-de-perseguir-atropelar-e-matar-motoboy-em-sp.ghtml\n",
            " • https://g1.globo.com/sp/sao-paulo/noticia/2025/05/31/apos-10-meses-justica-manda-soltar-motorista-do-porsche-amarelo-acusado-de-perseguir-atropelar-e-matar-motoboy-em-sp.ghtml\n",
            " • https://g1.globo.com/playlist/videos-para-assistir-agora.ghtml\n",
            " • https://g1.globo.com/playlist/videos-para-assistir-agora.ghtml\n",
            " • https://g1.globo.com/rs/rio-grande-do-sul/noticia/2025/05/31/pf-identifica-lider-do-pcc-ligado-a-plano-contra-autoridades-e-assalto-de-r-14-mi-em-aeroporto-no-rs.ghtml\n",
            " • https://g1.globo.com/rs/rio-grande-do-sul/noticia/2025/05/31/pf-identifica-lider-do-pcc-ligado-a-plano-contra-autoridades-e-assalto-de-r-14-mi-em-aeroporto-no-rs.ghtml\n",
            " • https://g1.globo.com/index/feed/pagina-4.ghtml\n",
            " • https://g1.globo.com\n",
            " • https://g1.globo.com/principios-editoriais-do-grupo-globo.html\n",
            " • https://privacidade.globo.com/privacy-policy/\n",
            " • https://minhaconta.globo.com\n",
            " • https://globoads.globo.com/\n",
            " • https://g1.globo.com/economia/agronegocios/\n",
            " • https://g1.globo.com/economia/agronegocios/agro-de-gente-pra-gente/\n",
            " • https://g1.globo.com/economia/agronegocios/globo-rural/\n",
            " • https://g1.globo.com/carnaval/2025/\n",
            " • https://g1.globo.com/rj/rio-de-janeiro/carnaval/2025/\n",
            " • https://g1.globo.com/sp/sao-paulo/carnaval/2025/\n",
            " • https://g1.globo.com/pe/pernambuco/carnaval/2025/\n",
            " • https://g1.globo.com/mg/minas-gerais/carnaval/2025/\n",
            " • https://g1.globo.com/carros/\n",
            " • https://g1.globo.com/carros/motos/\n",
            " • https://g1.globo.com/carros/dinheiro-sobre-rodas/\n",
            " • https://g1.globo.com/ciencia/\n",
            " • https://g1.globo.com/ciencia-e-saude/viva-voce/\n",
            " • https://g1.globo.com/economia/\n",
            " • https://g1.globo.com/economia/bitcoin/\n",
            " • https://g1.globo.com/economia/calculadoras/\n",
            " • https://g1.globo.com/economia/dolar/\n",
            " • https://g1.globo.com/economia/educacao-financeira/\n",
            " • https://g1.globo.com/economia/imposto-de-renda/\n",
            " • https://g1.globo.com/economia/midia-e-marketing/\n",
            " • https://g1.globo.com/economia/open-banking/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 3: Lendo uma Tabela HTML e Convertendo para DataFrame\n",
        "\n",
        "Vamos buscar uma tabela em uma página de demonstração (por exemplo, uma “Tabela de Países e Capitais”) e converter o conteúdo em um DataFrame do Pandas. Em seguida, salvamos esse DataFrame em CSV."
      ],
      "metadata": {
        "id": "WD0IZHLBBIV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Definir a URL que contém a tabela que desejamos extrair\n",
        "url = \"https://www.w3schools.com/html/html_tables.asp\"\n",
        "\n",
        "# Requisitar o HTML da página\n",
        "response = requests.get(url, timeout=10)\n",
        "soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "# Localizar a tabela: neste exemplo, a tabela tem id=\"customers\"\n",
        "table = soup.find(\"table\", {\"id\": \"customers\"})\n",
        "\n",
        "# Extrair cabeçalhos (th) e linhas (tr > td)\n",
        "headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
        "rows = []\n",
        "for row_tag in table.find_all(\"tr\")[1:]:  # pular o cabeçalho\n",
        "    cells = [td.get_text(strip=True) for td in row_tag.find_all(\"td\")]\n",
        "    if cells:\n",
        "        rows.append(cells)\n",
        "\n",
        "# Criar DataFrame com Pandas\n",
        "df = pd.DataFrame(rows, columns=headers)\n",
        "print(\"DataFrame extraído da tabela HTML:\")\n",
        "print(df.head())\n",
        "\n",
        "# Salvar o DataFrame em CSV dentro do Colab\n",
        "csv_path = \"tabela_paises_capitais.csv\"\n",
        "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"Tabela salva como: {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUa2FJNuATJb",
        "outputId": "9a548271-af48-4542-d86d-5199f7d2a5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame extraído da tabela HTML:\n",
            "                        Company          Contact  Country\n",
            "0           Alfreds Futterkiste     Maria Anders  Germany\n",
            "1    Centro comercial Moctezuma  Francisco Chang   Mexico\n",
            "2                  Ernst Handel    Roland Mendel  Austria\n",
            "3                Island Trading    Helen Bennett       UK\n",
            "4  Laughing Bacchus Winecellars  Yoshi Tannamuri   Canada\n",
            "Tabela salva como: tabela_paises_capitais.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 1: Extração de Manchetes e Links de Notícias\n",
        "1. Defina a URL da página de notícias (por exemplo o G1): https://g1.globo.com/\n",
        "1. Use a biblioteca requests para obter o HTML da página.\n",
        "1. Crie um objeto BeautifulSoup (parser lxml ou html.parser) para analisar o conteúdo.\n",
        "1. Identifique as tags que contêm as manchetes principais (por exemplo, `<a class=\"feed-post-link\">`).\n",
        "1. Para cada elemento encontrado, extraia:\n",
        "  - O texto da manchete (`.get_text()`).\n",
        "  - O atributo href do link associado.\n",
        "\n",
        "1. Armazene cada par (“manchete”, “link”) em uma lista e, ao final, imprima quantos itens foram coletados.\n"
      ],
      "metadata": {
        "id": "ebAMj5bpCXpS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j6teZY8-CcnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 2: Coleta de Produtos e Categorias no “Web Scraper Test Site”\n",
        "\n",
        "1. Acesse a página inicial do site de teste: https://webscraper.io/test-sites/e-commerce/static\n",
        "\n",
        "2. Use `requests` para obter o HTML e crie o `BeautifulSoup`.\n",
        "3. Localize a lista de categorias exibida na coluna esquerda (por exemplo, links dentro de `<div class=\"sidebar-nav\">`).\n",
        "4. Para cada categoria (ex.: “Computers”, “Phones”), siga o link correspondente e repita os próximos passos dentro dessa categoria:  \n",
        "  1. Use `requests` para carregar o HTML da página de categoria.  \n",
        "  2. Crie o `BeautifulSoup` para essa página.  \n",
        "  3. Identifique o contêiner de cada produto (por exemplo, `<div class=\"thumbnail\">`).  \n",
        "  4. Extraia, para cada produto:  \n",
        "     - Nome do produto (texto dentro de `<a class=\"title\">`).  \n",
        "     - Preço (texto dentro de `<h4 class=\"price\">`).  \n",
        "     - URL da imagem principal (atributo `src` de `<img class=\"img-responsive\">`).  \n",
        "  5. Armazene cada produto num dicionário contendo `{\"nome\": ..., \"preco\": ..., \"imagem\": ...}`.\n",
        "5. Agrupe todos os dicionários de produtos em uma estrutura que mapeie cada nome de categoria para a lista de produtos daquela categoria.\n",
        "6. Ao final, imprima quantos produtos foram coletados em cada categoria.\n"
      ],
      "metadata": {
        "id": "6TjLmeESCib7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WgnPiV9FCxhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 3: Extração e Análise de Tabela de “Worldometers” (COVID-19)\n",
        "\n",
        "1. Defina a URL da página com estatísticas mundiais de COVID-19: https://www.worldometers.info/coronavirus/\n",
        "2. Use `requests` para obter o HTML e crie o `BeautifulSoup`.\n",
        "3. Localize a tabela principal de países identificada por `id=\"main_table_countries_today\"`.\n",
        "4. Extraia os cabeçalhos (`<th>`) dessa tabela para obter os nomes das colunas.\n",
        "5. Para cada linha de dados (`<tr>` a partir do `<tbody>`), extraia todas as células (`<td>`), convertendo texto numérico (removendo vírgulas e sinais) para tipo numérico (`int` ou `float`) quando necessário.\n",
        "6. Construa um `DataFrame` do Pandas usando as listas de cabeçalhos e linhas de valores.\n",
        "7. Calcule a média de cada coluna numérica (ex.: “Total Cases”, “Total Deaths”).\n",
        "8. Identifique qual coluna tem o maior valor médio e registre esse resultado (por exemplo, imprimindo:\n",
        "```\n",
        "Coluna com maior média: Total Cases (Média = 1.234.567,89)\n",
        "```\n",
        "9. Salve o `DataFrame` completo em um arquivo CSV chamado `covid_worldometers.csv`.\n"
      ],
      "metadata": {
        "id": "thFXSUTPCyBP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZgIEOXdCyVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 4: Análise de Tamanho de Conteúdo em Páginas da Wikipédia\n",
        "\n",
        "1. Crie um arquivo de texto `urls.txt` contendo ao menos 10 URLs de artigos da Wikipédia (uma em cada linha), por exemplo:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Web_scraping\n",
        "\n",
        "https://en.wikipedia.org/wiki/Data_mining\n",
        "\n",
        "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
        "\n",
        "https://en.wikipedia.org/wiki/Machine_learning\n",
        "\n",
        "https://en.wikipedia.org/wiki/Big_data\n",
        "\n",
        "https://en.wikipedia.org/wiki/Natural_language_processing\n",
        "\n",
        "https://en.wikipedia.org/wiki/Computer_vision\n",
        "\n",
        "https://en.wikipedia.org/wiki/Deep_learning\n",
        "\n",
        "https://en.wikipedia.org/wiki/Neural_network\n",
        "\n",
        "https://en.wikipedia.org/wiki/Graph_neural_network\n",
        "\n",
        "2. No script, abra `urls.txt` e — linha a linha — armazene cada URL em uma lista `urls`.\n",
        "3. Para cada `url` em `urls`:  \n",
        "  1. Use `requests` para obter o HTML da página.  \n",
        "  2. Verifique se `response.status_code == 200`; caso contrário, registre erro e pule para a próxima URL.  \n",
        "  3. Crie um `BeautifulSoup` com o HTML retornado.  \n",
        "  4. Localize o elemento principal de conteúdo da Wikipédia (por exemplo, `<div id=\"mw-content-text\">`).  \n",
        "  5. Extraia todo o texto desse elemento usando `element.get_text(separator=\" \", strip=True)`.  \n",
        "  6. Calcule o tamanho em caracteres desse texto (`len(texto)`).  \n",
        "  7. Guarde no dicionário `resultados` a associação:  \n",
        "    ```\n",
        "    resultados[url] = tamanho_em_caracteres\n",
        "    ```\n",
        "4. Após processar todas as URLs, salve o dicionário `resultados` em um arquivo JSON chamado `tamanhos_wikipedia.json`.\n",
        "5. Imprima quantas páginas foram processadas com sucesso e liste as três URLs com maior quantidade de caracteres extraídos."
      ],
      "metadata": {
        "id": "xzpz-c7bCyr3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdsp9zo-Cy97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 5: Extração e Download de Imagens de um Blog\n",
        "\n",
        "1. Defina a URL de um blog ou página de conteúdo que possua várias imagens (por exemplo, https://blog.mozilla.org/).\n",
        "2. Use `requests` para obter o HTML e crie o `BeautifulSoup`.  \n",
        "3. Localize todas as tags `<img>` na página.  \n",
        "4. Para cada tag `<img>`, extraia:  \n",
        "  - O atributo `src` (URL da imagem).  \n",
        "  - O atributo `alt` (texto alternativo), caso exista.  \n",
        "5. Normalize cada `src` para obter a URL completa (caso seja um caminho relativo).  \n",
        "6. Use `requests` novamente para baixar cada imagem (`response = requests.get(url_imagem, timeout=10)`).  \n",
        "7. Salve cada arquivo de imagem localmente em uma pasta `imagens/`, mantendo o nome original ou gerando um nome único.  \n",
        "8. Imprima quantas imagens foram encontradas e quantas foram baixadas com sucesso.  "
      ],
      "metadata": {
        "id": "2DFR10VSDxEj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDb0F9IQDxUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}