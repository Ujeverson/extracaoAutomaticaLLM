{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extração Automatizada de Dados - Agentes Inteligentes INF/UFG\n",
        "### Prof. Otávio Calaça Xavier\n",
        "----\n",
        "\n",
        "## BeautifulSoup"
      ],
      "metadata": {
        "id": "Wf49sq3L-PAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo Prático com Código\n",
        "\n",
        "A seguir, vamos ver um script básico que:\n",
        "\n",
        "1. Solicita uma página via requests\n",
        "\n",
        "1. Faz o parsing com BeautifulSoup\n",
        "\n",
        "1. Localiza elementos específicos (títulos, preços, avaliações)\n",
        "\n",
        "1. Exibe no console quantos itens foram encontrados\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XKWg0Xhh-m4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://books.toscrape.com\"\n",
        "html = requests.get(url, timeout=10).text\n",
        "soup = BeautifulSoup(html, \"lxml\")  # ou \"html.parser\"\n",
        "\n",
        "# Encontrar todos os títulos\n",
        "titles = [h3.a[\"title\"] for h3 in soup.select(\"article.product_pod h3\")]\n",
        "print(len(titles), \"livros encontrados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEu_cNBY-ltf",
        "outputId": "82b55bbb-9a0b-49b0-9264-856b11f44207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 livros encontrados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2 — Extraindo Todos os Links de uma Página\n",
        "\n",
        "Vamos buscar todos os URLs (atributo href) encontrados em tags `<a>` de uma página e exibi-los em uma lista. Isso pode ser útil para coletar todas as páginas de um determinado domínio ou encontrar links de download."
      ],
      "metadata": {
        "id": "yJBnDa9V_D2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Definir a URL que queremos analisar\n",
        "url = \"https://g1.globo.com\"\n",
        "\n",
        "# Fazer a requisição HTTP para obter o HTML\n",
        "response = requests.get(url, timeout=10)\n",
        "html_content = response.text\n",
        "\n",
        "# Criar o objeto BeautifulSoup usando o parser lxml\n",
        "soup = BeautifulSoup(html_content, \"lxml\")\n",
        "\n",
        "# Encontrar todas as tags <a> que possuem atributo 'href'\n",
        "links = []\n",
        "for tag in soup.find_all(\"a\", href=True):\n",
        "    href = tag[\"href\"]\n",
        "    # (Opcional) Filtrar apenas links que começam com \"http\"\n",
        "    if href.startswith(\"http\"):\n",
        "        links.append(href)\n",
        "\n",
        "# Exibir quantos links foram encontrados e imprimir alguns exemplos\n",
        "print(f\"Total de links absolutos encontrados: {len(links)}\")\n",
        "print(\"Exemplos de links:\")\n",
        "for link in links[:50]:\n",
        "    print(\" •\", link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtI1adKbAPFU",
        "outputId": "3d15ea59-4f94-49f3-b97a-4e5637dd8dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de links absolutos encontrados: 675\n",
            "Exemplos de links:\n",
            " • https://g1.globo.com/\n",
            " • https://g1.globo.com/politica/noticia/2025/06/06/zambelli-pode-perder-mandato-por-decisao-da-camara-ou-da-justica-eleitoral-entenda.ghtml\n",
            " • https://g1.globo.com/politica/blog/natuza-nery/post/2025/06/06/zambelli-italia-autoridades-perseguicao-politica.ghtml\n",
            " • https://g1.globo.com/politica/noticia/2025/06/06/carla-zambelli-stf-mantem-por-unanimidade-condenacao-a-10-anos-por-invasao-a-sistema-do-cnj.ghtml\n",
            " • https://g1.globo.com/educacao/noticia/2025/06/06/enem-2025-inscricoes-sao-prorrogadas-ate-13-de-junho.ghtml\n",
            " • https://g1.globo.com/educacao/noticia/2025/06/06/enem-2025-inscricoes-sao-prorrogadas-ate-13-de-junho.ghtml\n",
            " • https://g1.globo.com/rj/rio-de-janeiro/noticia/2025/06/07/moradores-do-santo-amaro-relatam-tiroteio-em-acao-durante-festa-junina.ghtml\n",
            " • https://g1.globo.com/politica/noticia/2025/06/07/lula-defende-viagens-ao-exterior-e-diz-desconhecer-custos-estamos-fazendo-o-que-todo-presidente-precisa-fazer.ghtml\n",
            " • https://g1.globo.com/politica/noticia/2025/06/07/lula-defende-viagens-ao-exterior-e-diz-desconhecer-custos-estamos-fazendo-o-que-todo-presidente-precisa-fazer.ghtml\n",
            " • https://g1.globo.com/politica/noticia/2025/06/07/franca-vai-investir-r-100-bilhoes-no-brasil-ate-2030-diz-lula.ghtml\n",
            " • https://g1.globo.com/mg/minas-gerais/noticia/2025/06/07/agressao-a-bebe-confundida-com-boneca-reborn-o-que-se-sabe-sobre-o-caso.ghtml\n",
            " • https://g1.globo.com/mg/minas-gerais/noticia/2025/06/07/agressao-a-bebe-confundida-com-boneca-reborn-o-que-se-sabe-sobre-o-caso.ghtml\n",
            " • https://g1.globo.com/mundo/ucrania-russia/noticia/2025/06/07/russia-usou-pneus-para-camuflar-avioes-de-combate-e-tentar-enganar-drones-da-ucrania-entenda-por-que-nao-funcionou.ghtml\n",
            " • https://g1.globo.com/mundo/ucrania-russia/noticia/2025/06/07/russia-usou-pneus-para-camuflar-avioes-de-combate-e-tentar-enganar-drones-da-ucrania-entenda-por-que-nao-funcionou.ghtml\n",
            " • https://g1.globo.com/mundo/noticia/2025/06/07/menina-de-5-anos-sobrevive-a-ataque-de-israel-a-predio-de-gaza.ghtml\n",
            " • https://g1.globo.com/mundo/noticia/2025/06/07/menina-de-5-anos-sobrevive-a-ataque-de-israel-a-predio-de-gaza.ghtml\n",
            " • https://g1.globo.com/index/feed/pagina-4.ghtml\n",
            " • https://g1.globo.com\n",
            " • https://g1.globo.com/principios-editoriais-do-grupo-globo.html\n",
            " • https://privacidade.globo.com/privacy-policy/\n",
            " • https://minhaconta.globo.com\n",
            " • https://globoads.globo.com/\n",
            " • https://g1.globo.com/economia/agronegocios/\n",
            " • https://g1.globo.com/economia/agronegocios/agro-de-gente-pra-gente/\n",
            " • https://g1.globo.com/economia/agronegocios/globo-rural/\n",
            " • https://g1.globo.com/carnaval/2025/\n",
            " • https://g1.globo.com/rj/rio-de-janeiro/carnaval/2025/\n",
            " • https://g1.globo.com/sp/sao-paulo/carnaval/2025/\n",
            " • https://g1.globo.com/pe/pernambuco/carnaval/2025/\n",
            " • https://g1.globo.com/mg/minas-gerais/carnaval/2025/\n",
            " • https://g1.globo.com/carros/\n",
            " • https://g1.globo.com/carros/motos/\n",
            " • https://g1.globo.com/carros/dinheiro-sobre-rodas/\n",
            " • https://g1.globo.com/ciencia/\n",
            " • https://g1.globo.com/ciencia-e-saude/viva-voce/\n",
            " • https://g1.globo.com/economia/\n",
            " • https://g1.globo.com/economia/bitcoin/\n",
            " • https://g1.globo.com/economia/calculadoras/\n",
            " • https://g1.globo.com/economia/dolar/\n",
            " • https://g1.globo.com/economia/educacao-financeira/\n",
            " • https://g1.globo.com/economia/imposto-de-renda/\n",
            " • https://g1.globo.com/economia/midia-e-marketing/\n",
            " • https://g1.globo.com/economia/open-banking/\n",
            " • https://g1.globo.com/educacao/\n",
            " • https://g1.globo.com/educacao/enem/2024/\n",
            " • https://g1.globo.com/educacao/guia-de-carreiras/\n",
            " • https://especiais.g1.globo.com/educacao/enem/2023/simula/\n",
            " • http://especiais.g1.globo.com/educacao/guia-de-carreiras/2017/teste-vocacional/\n",
            " • https://g1.globo.com/educacao/universidades.html\n",
            " • https://g1.globo.com/empreendedorismo/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 3: Lendo uma Tabela HTML e Convertendo para DataFrame\n",
        "\n",
        "Vamos buscar uma tabela em uma página de demonstração (por exemplo, uma “Tabela de Países e Capitais”) e converter o conteúdo em um DataFrame do Pandas. Em seguida, salvamos esse DataFrame em CSV."
      ],
      "metadata": {
        "id": "WD0IZHLBBIV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Definir a URL que contém a tabela que desejamos extrair\n",
        "url = \"https://www.w3schools.com/html/html_tables.asp\"\n",
        "\n",
        "# Requisitar o HTML da página\n",
        "response = requests.get(url, timeout=10)\n",
        "soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "# Localizar a tabela: neste exemplo, a tabela tem id=\"customers\"\n",
        "table = soup.find(\"table\", {\"id\": \"customers\"})\n",
        "\n",
        "# Extrair cabeçalhos (th) e linhas (tr > td)\n",
        "headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
        "rows = []\n",
        "for row_tag in table.find_all(\"tr\")[1:]:  # pular o cabeçalho\n",
        "    cells = [td.get_text(strip=True) for td in row_tag.find_all(\"td\")]\n",
        "    if cells:\n",
        "        rows.append(cells)\n",
        "\n",
        "# Criar DataFrame com Pandas\n",
        "df = pd.DataFrame(rows, columns=headers)\n",
        "print(\"DataFrame extraído da tabela HTML:\")\n",
        "print(df.head())\n",
        "\n",
        "# Salvar o DataFrame em CSV dentro do Colab\n",
        "csv_path = \"tabela_paises_capitais.csv\"\n",
        "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"Tabela salva como: {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUa2FJNuATJb",
        "outputId": "87fc4968-a1b1-4a1d-faa7-f1c038488168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame extraído da tabela HTML:\n",
            "                        Company          Contact  Country\n",
            "0           Alfreds Futterkiste     Maria Anders  Germany\n",
            "1    Centro comercial Moctezuma  Francisco Chang   Mexico\n",
            "2                  Ernst Handel    Roland Mendel  Austria\n",
            "3                Island Trading    Helen Bennett       UK\n",
            "4  Laughing Bacchus Winecellars  Yoshi Tannamuri   Canada\n",
            "Tabela salva como: tabela_paises_capitais.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 1: Extração de Manchetes e Links de Notícias\n",
        "1. Defina a URL da página de notícias (por exemplo o G1): https://g1.globo.com/\n",
        "1. Use a biblioteca requests para obter o HTML da página.\n",
        "1. Crie um objeto BeautifulSoup (parser lxml ou html.parser) para analisar o conteúdo.\n",
        "1. Identifique as tags que contêm as manchetes principais (por exemplo, `<a class=\"feed-post-link\">`).\n",
        "1. Para cada elemento encontrado, extraia:\n",
        "  - O texto da manchete (`.get_text()`).\n",
        "  - O atributo href do link associado.\n",
        "\n",
        "1. Armazene cada par (“manchete”, “link”) em uma lista e, ao final, imprima quantos itens foram coletados.\n"
      ],
      "metadata": {
        "id": "ebAMj5bpCXpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://g1.globo.com/\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Erro ao acessar a página: {e}\")\n",
        "    exit()\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"lxml\")\n",
        "headlines_and_links = []\n",
        "\n",
        "for link_tag in soup.find_all(\"a\", class_=\"feed-post-link\"):\n",
        "    headline = link_tag.get_text(strip=True)\n",
        "    link = link_tag[\"href\"]\n",
        "\n",
        "    if headline and href:\n",
        "        headlines_and_links.append({\"manchete\": headline, \"link\": href})\n",
        "\n",
        "print(f\"Total de manchetes e links coletados: {len(headlines_and_links)}\")\n",
        "\n",
        "for item in headlines_and_links:\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Manchete: {item['manchete']}\")\n",
        "    print(f\"Link: {item['link']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "j6teZY8-CcnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6fc956-6c10-46ff-e4cf-f465a762d9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de manchetes e links coletados: 7\n",
            "--------------------\n",
            "Manchete: Zambelli pode perder mandato por decisão da Câmara ou da Justiça Eleitoral; entenda\n",
            "Link: #\n",
            "\n",
            "--------------------\n",
            "Manchete: Enem 2025: inscrições são prorrogadas até 13 de junho\n",
            "Link: #\n",
            "\n",
            "--------------------\n",
            "Manchete: Moradores de comunidade no Rio relatam tiroteio durante festa junina\n",
            "Link: #\n",
            "\n",
            "--------------------\n",
            "Manchete: Lula defende viagens ao exterior e diz desconhecer custos\n",
            "Link: #\n",
            "\n",
            "--------------------\n",
            "Manchete: Homem agride bebê achando que era boneca 'reborn'; entenda o caso\n",
            "Link: #\n",
            "\n",
            "--------------------\n",
            "Manchete: Rússia usou pneus para enganar drones da Ucrânia, mas não funcionou...\n",
            "Link: #\n",
            "\n",
            "--------------------\n",
            "Manchete: Criança de 5 anos viu os pais morrerem em ataque de Israel\n",
            "Link: #\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFa6wNlKc8mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 2: Coleta de Produtos e Categorias no “Web Scraper Test Site”\n",
        "\n",
        "1. Acesse a página inicial do site de teste: https://webscraper.io/test-sites/e-commerce/static\n",
        "\n",
        "2. Use `requests` para obter o HTML e crie o `BeautifulSoup`.\n",
        "3. Localize a lista de categorias exibida na coluna esquerda (por exemplo, links dentro de `<div class=\"sidebar-nav\">`).\n",
        "4. Para cada categoria (ex.: “Computers”, “Phones”), siga o link correspondente e repita os próximos passos dentro dessa categoria:  \n",
        "  1. Use `requests` para carregar o HTML da página de categoria.  \n",
        "  2. Crie o `BeautifulSoup` para essa página.  \n",
        "  3. Identifique o contêiner de cada produto (por exemplo, `<div class=\"thumbnail\">`).  \n",
        "  4. Extraia, para cada produto:  \n",
        "     - Nome do produto (texto dentro de `<a class=\"title\">`).  \n",
        "     - Preço (texto dentro de `<h4 class=\"price\">`).  \n",
        "     - URL da imagem principal (atributo `src` de `<img class=\"img-responsive\">`).  \n",
        "  5. Armazene cada produto num dicionário contendo `{\"nome\": ..., \"preco\": ..., \"imagem\": ...}`.\n",
        "5. Agrupe todos os dicionários de produtos em uma estrutura que mapeie cada nome de categoria para a lista de produtos daquela categoria.\n",
        "6. Ao final, imprima quantos produtos foram coletados em cada categoria.\n"
      ],
      "metadata": {
        "id": "6TjLmeESCib7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "from lxml import html\n",
        "\n",
        "\n",
        "BASE_URL = \"https://webscraper.io\"\n",
        "HOME_URL = f\"{BASE_URL}/test-sites/e-commerce/static\"\n",
        "\n",
        "# Função para obter o conteúdo HTML de uma página\n",
        "def get_soup(url):\n",
        "    response = requests.get(url)\n",
        "    return BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Função para extrair categorias do menu lateral\n",
        "def extrair_categorias():\n",
        "    url_inicial = f\"{BASE_URL}/test-sites/e-commerce/static\"\n",
        "    soup = get_soup(url_inicial)\n",
        "\n",
        "    categorias = {}\n",
        "    sidebar = soup.find(\"div\", class_=\"sidebar-nav\")\n",
        "    for link in sidebar.find_all(\"a\", href=True):\n",
        "        nome = link.get_text(strip=True)\n",
        "        href = link[\"href\"]\n",
        "        if href.startswith(\"/test-sites/e-commerce/static/\"):\n",
        "            categorias[nome] = BASE_URL + href\n",
        "\n",
        "    return categorias\n",
        "\n",
        "categorias = extrair_categorias()\n",
        "\n",
        "print(\"Categorias encontradas:\")\n",
        "for nome, url in categorias.items():\n",
        "    print(f\"- {nome}: {url}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Função para extrair produtos de uma página de categoria\n",
        "def extrair_produtos(url_categoria):\n",
        "    soup = get_soup(url_categoria)\n",
        "    produtos = []\n",
        "\n",
        "    for item in soup.find_all(\"div\", class_=\"thumbnail\"):\n",
        "        nome = item.find(\"a\", class_=\"title\").get_text(strip=True)\n",
        "        preco = item.find(\"h4\", class_=\"price\").get_text(strip=True)\n",
        "        imagem = BASE_URL + item.find(\"img\", class_=\"img-responsive\")[\"src\"]\n",
        "\n",
        "        produtos.append({\n",
        "            \"nome\": nome,\n",
        "            \"preco\": preco,\n",
        "            \"imagem\": imagem\n",
        "        })\n",
        "\n",
        "    return produtos\n",
        "\n",
        "\n",
        "def coletar_dados():\n",
        "    categorias = extrair_categorias()\n",
        "    dados = defaultdict(list)\n",
        "\n",
        "    for nome_cat, url_cat in categorias.items():\n",
        "        print(f\"Coletando da categoria: {nome_cat}\")\n",
        "        produtos = extrair_produtos(url_cat)\n",
        "        dados[nome_cat].extend(produtos)\n",
        "\n",
        "    return dados\n",
        "\n",
        "# Executando a coleta\n",
        "dados_coletados = coletar_dados()\n",
        "print(\"-\" * 30)\n",
        "print(\"Dados coletatos\")\n",
        "print(dados_coletados)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "# Exibindo resumo por categoria\n",
        "print(\"\\nResumo por categoria:\")\n",
        "for categoria, produtos in dados_coletados.items():\n",
        "    print(f\"- {categoria}: {len(produtos)} produtos\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def get_soup(url):\n",
        "    response = requests.get(url)\n",
        "    return BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Filtrando apenas as categorias desejadas\n",
        "def extrair_categorias_desejadas():\n",
        "    soup = get_soup(HOME_URL)\n",
        "    categorias_desejadas = [\"Computers\", \"Phones\"]\n",
        "    categorias = {}\n",
        "\n",
        "    # Navega pelo menu lateral\n",
        "    for link in soup.select(\"div.sidebar-nav a[href]\"):\n",
        "        nome = link.get_text(strip=True)\n",
        "        href = link[\"href\"]\n",
        "\n",
        "        if any(nome.startswith(desejada) for desejada in categorias_desejadas):\n",
        "            categorias[nome] = BASE_URL + href\n",
        "    return categorias\n",
        "\n",
        "categorias = extrair_categorias_desejadas()\n",
        "\n",
        "print(\"Categorias selecionadas:\")\n",
        "for nome, url in categorias.items():\n",
        "    print(f\"- {nome}: {url}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def extrair_subcategorias(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    menu = soup.find(\"ul\", id=\"side-menu\")\n",
        "    subcategorias = []\n",
        "\n",
        "    for li in menu.find_all(\"li\"):\n",
        "        span = li.find(\"span\")\n",
        "        if span:\n",
        "            texto = span.get_text(strip=True)\n",
        "            subcategorias.append(texto)\n",
        "\n",
        "    return subcategorias\n",
        "\n",
        "def extrair_produtos(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    produtos = []\n",
        "\n",
        "    for caption in soup.find_all(\"div\", class_=\"caption\"):\n",
        "        nome_tag = caption.find(\"a\", itemprop=\"name\")\n",
        "        preco_tag = caption.find(\"span\", itemprop=\"price\")\n",
        "\n",
        "        if nome_tag and preco_tag:\n",
        "            nome = nome_tag.get_text(strip=True)\n",
        "            preco = preco_tag.get_text(strip=True)\n",
        "            produtos.append({\n",
        "                \"nome\": nome,\n",
        "                \"preco\": preco\n",
        "            })\n",
        "\n",
        "    return produtos\n",
        "\n",
        "# URLs específicas\n",
        "url_home = \"https://webscraper.io/test-sites/e-commerce/static\"\n",
        "url_laptops = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops\"\n",
        "url_tablets = \"https://webscraper.io/test-sites/e-commerce/static/computers/tablets\"\n",
        "url_touch = \"https://webscraper.io/test-sites/e-commerce/static/phones/touch\"\n",
        "\n",
        "# Subcategorias\n",
        "print(\"\\n Subcategorias disponíveis:\")\n",
        "subcategorias = extrair_subcategorias(url_home)\n",
        "for item in subcategorias:\n",
        "    print(\"-\", item)\n",
        "\n",
        "# Extração dos produtos por subcategoria\n",
        "produtos_laptops = extrair_produtos(url_laptops)\n",
        "produtos_tablets = extrair_produtos(url_tablets)\n",
        "produtos_touch = extrair_produtos(url_touch)\n",
        "\n",
        "# Exibição dos produtos\n",
        "print(\"\\n Produtos - LAPTOPS:\")\n",
        "for p in produtos_laptops:\n",
        "    print(f\"{p['nome']} | {p['preco']}\")\n",
        "\n",
        "print(\"\\n Produtos - TABLETS:\")\n",
        "for p in produtos_tablets:\n",
        "    print(f\"{p['nome']} | {p['preco']}\")\n",
        "\n",
        "print(\"\\n Produtos - TOUCH PHONES:\")\n",
        "for p in produtos_touch:\n",
        "    print(f\"{p['nome']} | {p['preco']}\")\n"
      ],
      "metadata": {
        "id": "WgnPiV9FCxhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed0049d-f665-45e6-ff8a-2a8270f99a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorias encontradas:\n",
            "- Computers: https://webscraper.io/test-sites/e-commerce/static/computers\n",
            "- Phones: https://webscraper.io/test-sites/e-commerce/static/phones\n",
            "------------------------------\n",
            "Coletando da categoria: Computers\n",
            "Coletando da categoria: Phones\n",
            "------------------------------\n",
            "Dados coletatos\n",
            "defaultdict(<class 'list'>, {'Computers': [{'nome': 'ThinkPad X230', 'preco': '$1244.99', 'imagem': 'https://webscraper.io/images/test-sites/e-commerce/items/cart2.png'}, {'nome': 'Asus VivoBook...', 'preco': '$729', 'imagem': 'https://webscraper.io/images/test-sites/e-commerce/items/cart2.png'}, {'nome': 'Packard 255 G2', 'preco': '$416.99', 'imagem': 'https://webscraper.io/images/test-sites/e-commerce/items/cart2.png'}], 'Phones': [{'nome': 'Iphone', 'preco': '$899.99', 'imagem': 'https://webscraper.io/images/test-sites/e-commerce/items/cart2.png'}, {'nome': 'Nokia 123', 'preco': '$24.99', 'imagem': 'https://webscraper.io/images/test-sites/e-commerce/items/cart2.png'}, {'nome': 'LG Optimus', 'preco': '$57.99', 'imagem': 'https://webscraper.io/images/test-sites/e-commerce/items/cart2.png'}]})\n",
            "------------------------------\n",
            "\n",
            "Resumo por categoria:\n",
            "- Computers: 3 produtos\n",
            "- Phones: 3 produtos\n",
            "------------------------------\n",
            "Categorias selecionadas:\n",
            "- Computers: https://webscraper.io/test-sites/e-commerce/static/computers\n",
            "- Phones: https://webscraper.io/test-sites/e-commerce/static/phones\n",
            "------------------------------\n",
            "\n",
            " Subcategorias disponíveis:\n",
            "- Home\n",
            "- Computers\n",
            "- Phones\n",
            "\n",
            " Produtos - LAPTOPS:\n",
            "Packard 255 G2 | $416.99\n",
            "Aspire E1-510 | $306.99\n",
            "ThinkPad T540p | $1178.99\n",
            "ProBook | $739.99\n",
            "ThinkPad X240 | $1311.99\n",
            "Aspire E1-572G | $581.99\n",
            "\n",
            " Produtos - TABLETS:\n",
            "Lenovo IdeaTab | $69.99\n",
            "Acer Iconia | $96.99\n",
            "Asus MeMO Pad | $102.99\n",
            "Amazon Kindle | $103.99\n",
            "iPad Mini Reti... | $537.99\n",
            "IdeaTab A3500L | $88.99\n",
            "\n",
            " Produtos - TOUCH PHONES:\n",
            "Nokia 123 | $24.99\n",
            "LG Optimus | $57.99\n",
            "Samsung Galaxy | $93.99\n",
            "Nokia X | $109.99\n",
            "Sony Xperia | $118.99\n",
            "Ubuntu Edge | $499.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 3: Extração e Análise de Tabela de “Worldometers” (COVID-19)\n",
        "\n",
        "1. Defina a URL da página com estatísticas mundiais de COVID-19: https://www.worldometers.info/coronavirus/\n",
        "2. Use `requests` para obter o HTML e crie o `BeautifulSoup`.\n",
        "3. Localize a tabela principal de países identificada por `id=\"main_table_countries_today\"`.\n",
        "4. Extraia os cabeçalhos (`<th>`) dessa tabela para obter os nomes das colunas.\n",
        "5. Para cada linha de dados (`<tr>` a partir do `<tbody>`), extraia todas as células (`<td>`), convertendo texto numérico (removendo vírgulas e sinais) para tipo numérico (`int` ou `float`) quando necessário.\n",
        "6. Construa um `DataFrame` do Pandas usando as listas de cabeçalhos e linhas de valores.\n",
        "7. Calcule a média de cada coluna numérica (ex.: “Total Cases”, “Total Deaths”).\n",
        "8. Identifique qual coluna tem o maior valor médio e registre esse resultado (por exemplo, imprimindo:\n",
        "```\n",
        "Coluna com maior média: Total Cases (Média = 1.234.567,89)\n",
        "```\n",
        "9. Salve o `DataFrame` completo em um arquivo CSV chamado `covid_worldometers.csv`.\n"
      ],
      "metadata": {
        "id": "thFXSUTPCyBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Item 1\n",
        "url = \"https://www.worldometers.info/coronavirus/\"\n",
        "print(\"Item 1: URL definida.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 2\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "print(\"Item 2: HTML baixado e BeautifulSoup criado.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 3\n",
        "tabela = soup.find(\"table\", id=\"main_table_countries_today\")\n",
        "print(\"Item 3: Tabela principal localizada.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 4\n",
        "cabecalhos = [th.get_text(strip=True) for th in tabela.find_all(\"thead\")[0].find_all(\"th\")]\n",
        "print(\"Item 4: Cabeçalhos extraídos:\", cabecalhos)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 5\n",
        "\n",
        "linhas = []\n",
        "for tr in tabela.tbody.find_all(\"tr\"):\n",
        "    colunas = tr.find_all(\"td\")\n",
        "    if not colunas:\n",
        "        continue\n",
        "    linha = []\n",
        "    for td in colunas:\n",
        "        texto = td.get_text(strip=True)\n",
        "        texto = re.sub(r\"[^\\d.,\\\\-]\", \"\", texto)\n",
        "        if texto in (\"\", \"-\"):\n",
        "            valor = None\n",
        "        elif \",\" in texto:\n",
        "            texto = texto.replace(\",\", \"\")\n",
        "            valor = float(texto) if \".\" in texto else int(texto)\n",
        "        else:\n",
        "            try:\n",
        "                valor = int(texto)\n",
        "            except:\n",
        "                try:\n",
        "                    valor = float(texto)\n",
        "                except:\n",
        "                    valor = texto\n",
        "        linha.append(valor)\n",
        "    linhas.append(linha)\n",
        "\n",
        "for i, td in enumerate(colunas):\n",
        "    texto = td.get_text(strip=True)\n",
        "    texto = re.sub(r\"[^\\d.,\\\\-]\", \"\", texto) if i > 0 else td.get_text(strip=True)\n",
        "\n",
        "print(f\"Item 5: {len(linhas)} linhas extraídas.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 6\n",
        "df = pd.DataFrame(linhas, columns=cabecalhos)\n",
        "df.dropna(axis=1, how='all', inplace=True)\n",
        "print(\"Item 6: DataFrame criado.\")\n",
        "print(df.head())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# Item 7\n",
        "medias = df.select_dtypes(include=[\"number\"]).mean()\n",
        "medias = medias/1000000\n",
        "print(\"Item 7: Médias calculadas (x 10000000)\")\n",
        "print(medias)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 8\n",
        "col_maior_media = medias.idxmax()\n",
        "val_maior_media = medias.max()\n",
        "print(f\"Item 8: Coluna com maior média: {col_maior_media} (Média = {val_maior_media:,.2f} milhões) \")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 9\n",
        "df.to_csv(\"covid_worldometers.csv\", index=False)\n",
        "print(\"Item 9: CSV salvo como covid_worldometers.csv.\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "4ZgIEOXdCyVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f55e0c-cf27-400a-857e-0b2b5166ffbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item 1: URL definida.\n",
            "------------------------------\n",
            "Item 2: HTML baixado e BeautifulSoup criado.\n",
            "------------------------------\n",
            "Item 3: Tabela principal localizada.\n",
            "------------------------------\n",
            "Item 4: Cabeçalhos extraídos: ['#', 'Country,Other', 'TotalCases', 'NewCases', 'TotalDeaths', 'NewDeaths', 'TotalRecovered', 'NewRecovered', 'ActiveCases', 'Serious,Critical', 'Tot\\xa0Cases/1M pop', 'Deaths/1M pop', 'TotalTests', 'Tests/1M pop', 'Population', 'Continent', '1 Caseevery X ppl', '1 Deathevery X ppl', '1 Testevery X ppl', 'New Cases/1M pop', 'New Deaths/1M pop', 'Active Cases/1M pop']\n",
            "------------------------------\n",
            "Item 5: 239 linhas extraídas.\n",
            "------------------------------\n",
            "Item 6: DataFrame criado.\n",
            "    # Country,Other  TotalCases  NewCases  TotalDeaths  NewDeaths  \\\n",
            "0 NaN          None   131889132       NaN    1695941.0        NaN   \n",
            "1 NaN          None   221500265       NaN    1553662.0        NaN   \n",
            "2 NaN          None   253406198       NaN    2101824.0        NaN   \n",
            "3 NaN          None    70200879       NaN    1367332.0        NaN   \n",
            "4 NaN          None    14895771       NaN      33015.0        NaN   \n",
            "\n",
            "   TotalRecovered  NewRecovered  ActiveCases  Serious,Critical  \\\n",
            "0     127665129.0         350.0    2528062.0            6095.0   \n",
            "1     205673091.0           NaN   14273512.0           14733.0   \n",
            "2     248754104.0         474.0    2550270.0            4453.0   \n",
            "3      66683585.0           NaN    2149962.0            8953.0   \n",
            "4      14752388.0           NaN     110368.0              31.0   \n",
            "\n",
            "   Tot Cases/1M pop  Deaths/1M pop  TotalTests  Tests/1M pop  Population  \\\n",
            "0               NaN            NaN         NaN           NaN         NaN   \n",
            "1               NaN            NaN         NaN           NaN         NaN   \n",
            "2               NaN            NaN         NaN           NaN         NaN   \n",
            "3               NaN            NaN         NaN           NaN         NaN   \n",
            "4               NaN            NaN         NaN           NaN         NaN   \n",
            "\n",
            "   1 Caseevery X ppl  1 Deathevery X ppl  1 Testevery X ppl  \\\n",
            "0                NaN                 NaN                NaN   \n",
            "1                NaN                 NaN                NaN   \n",
            "2                NaN                 NaN                NaN   \n",
            "3                NaN                 NaN                NaN   \n",
            "4                NaN                 NaN                NaN   \n",
            "\n",
            "   Active Cases/1M pop  \n",
            "0                  NaN  \n",
            "1                  NaN  \n",
            "2                  NaN  \n",
            "3                  NaN  \n",
            "4                  NaN  \n",
            "------------------------------\n",
            "Item 7: Médias calculadas (x 10000000)\n",
            "#                       0.000116\n",
            "TotalCases              8.846283\n",
            "NewCases                0.000000\n",
            "TotalDeaths             0.089881\n",
            "NewDeaths               0.000000\n",
            "TotalRecovered         10.009462\n",
            "NewRecovered            0.000305\n",
            "ActiveCases             0.262305\n",
            "Serious,Critical        0.001208\n",
            "Tot Cases/1M pop        0.202461\n",
            "Deaths/1M pop           0.001276\n",
            "TotalTests             32.988288\n",
            "Tests/1M pop            2.155978\n",
            "Population             34.694040\n",
            "1 Caseevery X ppl       0.000402\n",
            "1 Deathevery X ppl      0.014393\n",
            "1 Testevery X ppl       0.000011\n",
            "Active Cases/1M pop     0.033323\n",
            "dtype: float64\n",
            "------------------------------\n",
            "Item 8: Coluna com maior média: Population (Média = 34.69 milhões) \n",
            "------------------------------\n",
            "Item 9: CSV salvo como covid_worldometers.csv.\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 4: Análise de Tamanho de Conteúdo em Páginas da Wikipédia\n",
        "\n",
        "1. Crie um arquivo de texto `urls.txt` contendo ao menos 10 URLs de artigos da Wikipédia (uma em cada linha), por exemplo:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Web_scraping\n",
        "\n",
        "https://en.wikipedia.org/wiki/Data_mining\n",
        "\n",
        "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
        "\n",
        "https://en.wikipedia.org/wiki/Machine_learning\n",
        "\n",
        "https://en.wikipedia.org/wiki/Big_data\n",
        "\n",
        "https://en.wikipedia.org/wiki/Natural_language_processing\n",
        "\n",
        "https://en.wikipedia.org/wiki/Computer_vision\n",
        "\n",
        "https://en.wikipedia.org/wiki/Deep_learning\n",
        "\n",
        "https://en.wikipedia.org/wiki/Neural_network\n",
        "\n",
        "https://en.wikipedia.org/wiki/Graph_neural_network\n",
        "\n",
        "2. No script, abra `urls.txt` e — linha a linha — armazene cada URL em uma lista `urls`.\n",
        "3. Para cada `url` em `urls`:  \n",
        "  1. Use `requests` para obter o HTML da página.  \n",
        "  2. Verifique se `response.status_code == 200`; caso contrário, registre erro e pule para a próxima URL.  \n",
        "  3. Crie um `BeautifulSoup` com o HTML retornado.  \n",
        "  4. Localize o elemento principal de conteúdo da Wikipédia (por exemplo, `<div id=\"mw-content-text\">`).  \n",
        "  5. Extraia todo o texto desse elemento usando `element.get_text(separator=\" \", strip=True)`.  \n",
        "  6. Calcule o tamanho em caracteres desse texto (`len(texto)`).  \n",
        "  7. Guarde no dicionário `resultados` a associação:  \n",
        "    ```\n",
        "    resultados[url] = tamanho_em_caracteres\n",
        "    ```\n",
        "4. Após processar todas as URLs, salve o dicionário `resultados` em um arquivo JSON chamado `tamanhos_wikipedia.json`.\n",
        "5. Imprima quantas páginas foram processadas com sucesso e liste as três URLs com maior quantidade de caracteres extraídos."
      ],
      "metadata": {
        "id": "xzpz-c7bCyr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# Item 2\n",
        "with open(\"urls.txt\", \"r\") as f:\n",
        "    urls = [linha.strip() for linha in f.readlines()]\n",
        "print(\"Item 2: URLs carregadas\")\n",
        "for url in urls:\n",
        "    print(url)\n",
        "print(\"-\" * 30)\n",
        "# Item 3\n",
        "resultados = {}\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Erro {response.status_code} em: {url} \\n\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        conteudo = soup.find(\"div\", id=\"mw-content-text\")\n",
        "        texto = conteudo.get_text(separator=\" \", strip=True)\n",
        "        resultados[url] = len(texto)\n",
        "        print(f\"{url} - {len(texto)} caracteres \\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro em {url}: {e} \\n\")\n",
        "print(\"-\" * 30)\n",
        "# Item 4\n",
        "with open(\"tamanhos_wikipedia.json\", \"w\") as f:\n",
        "    json.dump(resultados, f, indent=2)\n",
        "print(\"Item 4: JSON salvo.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 5\n",
        "print(f\"\\nItem 5: {len(resultados)} páginas processadas com sucesso.\")\n",
        "top3 = sorted(resultados.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "print(\"Top 3 maiores:\")\n",
        "for url, tamanho in top3:\n",
        "    print(f\"{url} - {tamanho} caracteres\")"
      ],
      "metadata": {
        "id": "fdsp9zo-Cy97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfae026d-aa08-4a3b-aaa9-33b5b1fe90bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item 2: URLs carregadas\n",
            "https://en.wikipedia.org/wiki/Web_scraping\n",
            "\n",
            "https://en.wikipedia.org/wiki/Data_mining\n",
            "\n",
            "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "\n",
            "https://en.wikipedia.org/wiki/Machine_learning\n",
            "\n",
            "https://en.wikipedia.org/wiki/Big_data\n",
            "\n",
            "https://en.wikipedia.org/wiki/Natural_language_processing\n",
            "\n",
            "https://en.wikipedia.org/wiki/Computer_vision\n",
            "\n",
            "https://en.wikipedia.org/wiki/Deep_learning\n",
            "\n",
            "https://en.wikipedia.org/wiki/Neural_network\n",
            "\n",
            "https://en.wikipedia.org/wiki/Graph_neural_network\n",
            "------------------------------\n",
            "https://en.wikipedia.org/wiki/Web_scraping - 25713 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Data_mining - 42504 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Artificial_intelligence - 214382 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Machine_learning - 121004 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Big_data - 111823 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Natural_language_processing - 48481 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Computer_vision - 58023 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Deep_learning - 132024 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Neural_network - 6160 caracteres \n",
            "\n",
            "Erro em : Invalid URL '': No scheme supplied. Perhaps you meant https://? \n",
            "\n",
            "https://en.wikipedia.org/wiki/Graph_neural_network - 39248 caracteres \n",
            "\n",
            "------------------------------\n",
            "Item 4: JSON salvo.\n",
            "------------------------------\n",
            "\n",
            "Item 5: 10 páginas processadas com sucesso.\n",
            "Top 3 maiores:\n",
            "https://en.wikipedia.org/wiki/Artificial_intelligence - 214382 caracteres\n",
            "https://en.wikipedia.org/wiki/Deep_learning - 132024 caracteres\n",
            "https://en.wikipedia.org/wiki/Machine_learning - 121004 caracteres\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 5: Extração e Download de Imagens de um Blog\n",
        "\n",
        "1. Defina a URL de um blog ou página de conteúdo que possua várias imagens (por exemplo, https://blog.mozilla.org/).\n",
        "2. Use `requests` para obter o HTML e crie o `BeautifulSoup`.  \n",
        "3. Localize todas as tags `<img>` na página.  \n",
        "4. Para cada tag `<img>`, extraia:  \n",
        "  - O atributo `src` (URL da imagem).  \n",
        "  - O atributo `alt` (texto alternativo), caso exista.  \n",
        "5. Normalize cada `src` para obter a URL completa (caso seja um caminho relativo).  \n",
        "6. Use `requests` novamente para baixar cada imagem (`response = requests.get(url_imagem, timeout=10)`).  \n",
        "7. Salve cada arquivo de imagem localmente em uma pasta `imagens/`, mantendo o nome original ou gerando um nome único.  \n",
        "8. Imprima quantas imagens foram encontradas e quantas foram baixadas com sucesso.  "
      ],
      "metadata": {
        "id": "2DFR10VSDxEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "# Item 1\n",
        "url = \"https://blog.mozilla.org/\"\n",
        "print(\"Item 1: URL definida:\", url)\n",
        "print()\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 2\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "print(\"Item 2: HTML baixado e BeautifulSoup criado.\")\n",
        "print()\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 3\n",
        "imagens = soup.find_all(\"img\")\n",
        "print(f\"Item 3: {len(imagens)} imagens encontradas.\")\n",
        "print()\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 4 e 5\n",
        "dados_imagens = []\n",
        "for img in imagens:\n",
        "    src = img.get(\"src\")\n",
        "    alt = img.get(\"alt\", \"\")\n",
        "    if src:\n",
        "        url_img = urljoin(url, src)\n",
        "        dados_imagens.append({\"src\": url_img, \"alt\": alt})\n",
        "print(\"Item 4 e 5: Exemplos de URLs de imagem: \\n\", dados_imagens[:3])\n",
        "\n",
        "print()\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Item 6 e 7\n",
        "os.makedirs(\"imagens\", exist_ok=True)\n",
        "sucesso = 0\n",
        "for i, img in enumerate(dados_imagens):\n",
        "    try:\n",
        "        nome = os.path.basename(urlparse(img[\"src\"]).path)\n",
        "        if not nome:\n",
        "            nome = f\"imagem_{i}.jpg\"\n",
        "        resp = requests.get(img[\"src\"], timeout=10)\n",
        "        with open(f\"imagens/{nome}\", \"wb\") as f:\n",
        "            f.write(resp.content)\n",
        "        sucesso += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao baixar {img['src']}: {e}\")\n",
        "print(f\"{len(dados_imagens)} imagens encontradas e {sucesso} baixadas com sucesso na pasta imagens.\")\n",
        "print()\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "BDb0F9IQDxUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9920a8c7-e4a8-474e-eea4-8c45e3d70b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item 1: URL definida: https://blog.mozilla.org/\n",
            "\n",
            "------------------------------\n",
            "Item 2: HTML baixado e BeautifulSoup criado.\n",
            "\n",
            "------------------------------\n",
            "Item 3: 31 imagens encontradas.\n",
            "\n",
            "------------------------------\n",
            "Item 4 e 5: Exemplos de URLs de imagem: \n",
            " [{'src': 'https://blog.mozilla.org/wp-content/themes/foxtail/assets/images/icons/search.svg', 'alt': 'search'}, {'src': 'https://blog.mozilla.org/wp-content/themes/foxtail/assets/images/icons/search.svg', 'alt': 'search'}, {'src': 'https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/05/Header-1080x720.jpg', 'alt': 'Illustration of a Firefox browser window with a purple search bar and the Firefox logo in the center, surrounded by playful icons for printing, downloading, taking screenshots, and extensions on a blue-to-purple gradient background.'}]\n",
            "\n",
            "------------------------------\n",
            "31 imagens encontradas e 31 baixadas com sucesso na pasta imagens.\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}